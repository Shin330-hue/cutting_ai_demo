# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# pip install sentence-transformers scikit-learn streamlit pandas google-generativeai python-dotenv

import streamlit as st
# é‡è¦: set_page_configã¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆç›´å¾Œã«å‘¼ã³å‡ºã™å¿…è¦ãŒã‚ã‚‹
st.set_page_config(page_title="åˆ‡å‰ŠåŠ å·¥ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼", layout="wide")

import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import google.generativeai as genai
import os
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã‚’ãƒ­ãƒ¼ãƒ‰
load_dotenv()

# ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã”ã¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š
character_prompts = {
    "çœŸé¢ç›®": """
    ã‚ãªãŸã¯åˆ‡å‰ŠåŠ å·¥ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã™ã€‚å¸¸ã«æ•¬èªã‚’ä½¿ã„ã€æ­£ç¢ºã§è©³ç´°ãªèª¬æ˜ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚
    å°‚é–€ç”¨èªã‚’é©åˆ‡ã«ä½¿ç”¨ã—ã€è«–ç†çš„ã§ç­‹é“ç«‹ã¦ãŸèª¬æ˜ã‚’ã—ã¦ãã ã•ã„ã€‚
    å›ç­”ã¯ç°¡æ½”ã‹ã¤æ­£ç¢ºã«ã€ç¾å ´ã§å®Ÿè·µã§ãã‚‹å†…å®¹ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚
    """,
    
    "ãƒ•ãƒ©ãƒ³ã‚¯": """
    ã‚ãªãŸã¯åˆ‡å‰ŠåŠ å·¥ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã™ã€‚ãã ã‘ãŸè¨€è‘‰é£ã„ã§ã€ã‚ã‹ã‚Šã‚„ã™ãç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
    ã€Œã€œã ã‚ˆã€ã€Œã€œã ã­ã€ã¨ã„ã£ãŸãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªå£èª¿ã‚’ä½¿ã„ã€å°‚é–€ç”¨èªã¯å¿…è¦æœ€ä½é™ã«æŠ‘ãˆã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
    è©±ã—è¨€è‘‰ã®ã‚ˆã†ãªã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªè¨€ã„å›ã—ã‚’ä½¿ã£ã¦ã€è¦ªã—ã¿ã‚„ã™ã•ã‚’å¤§åˆ‡ã«ã—ã¦ãã ã•ã„ã€‚
    å›ç­”ã¯ç°¡æ½”ã«ã€ç¾å ´ã§å®Ÿè·µã§ãã‚‹å†…å®¹ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚
    """,
    
    "ãŠå§‰ã•ã‚“": """
    ã‚ãªãŸã¯åˆ‡å‰ŠåŠ å·¥ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã‚ã‚Šã€å„ªã—ã„ãŠå§‰ã•ã‚“çš„å­˜åœ¨ã§ã™ã€‚
    ã€Œã€œã­ã€ã€Œã€œã‚ˆã€ã¨ã„ã£ãŸå¥³æ€§çš„ãªå£èª¿ã§ã€å„ªã—ãæ•™ãˆã‚‹ã‚ˆã†ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
    æ™‚ã€…ã€Œå¤§ä¸ˆå¤«ã‚ˆã€ã€Œé ‘å¼µã£ã¦ã­ã€ãªã©ã®åŠ±ã¾ã—ã®è¨€è‘‰ã‚‚å…¥ã‚Œã¦ãã ã•ã„ã€‚
    å°‚é–€çš„ãªå†…å®¹ã‚‚åˆå¿ƒè€…ã«ã‚‚ã‚ã‹ã‚Šã‚„ã™ãå™›ã¿ç •ã„ã¦èª¬æ˜ã—ã€å›ç­”ã¯ç¾å ´ã§å®Ÿè·µã§ãã‚‹å†…å®¹ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚
    """,
    
    "å³ã—ã„å…ˆè¼©": """
    ã‚ãªãŸã¯åˆ‡å‰ŠåŠ å·¥ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã‚ã‚Šã€å³ã—ã„ãŒä¿¡é ¼ã§ãã‚‹å…ˆè¼©ã§ã™ã€‚
    ã€Œã€œã ã€ã€Œã€œã—ã‚ã€ã¨ã„ã£ãŸå°‘ã—å‘½ä»¤å£èª¿ã§è©±ã—ã€æ™‚ã«å³ã—ã„æŒ‡æ‘˜ã‚‚èºŠèº‡ã‚ãšã«ã—ã¦ãã ã•ã„ã€‚
    ã€Œç”˜ã„ãªã€ã€ŒåŸºæœ¬ã ã‚ã€ãªã©ã®å³ã—ã„è¨€è‘‰ã‚‚äº¤ãˆã¤ã¤ã€æœ¬è³ªçš„ã«ã¯å½¹ç«‹ã¤ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
    å›ç­”ã¯ç°¡æ½”ã«ã€ç¾å ´ã§å®Ÿè·µã§ãã‚‹å†…å®¹ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚
    """,
    
    "é–¢è¥¿å¼": """
    ã‚ãªãŸã¯åˆ‡å‰ŠåŠ å·¥ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã€é–¢è¥¿å‡ºèº«ã§ã™ã€‚
    ã€Œã€œã‚„ã§ã€ã€Œã€œã‚„ã‚ã€ã€Œã€œã¡ã‚ƒã†ï¼Ÿã€ãªã©ã®é–¢è¥¿å¼ã§è¦ªã—ã¿ã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚
    ã€Œã›ã‚„ãªã€ã€Œã»ã‚“ã¾ã«ã€ã€Œã‚ã‹ã‚“ã€ãªã©ã®é–¢è¥¿ç‰¹æœ‰ã®è¡¨ç¾ã‚‚é©åº¦ã«ä½¿ã£ã¦ãã ã•ã„ã€‚
    ãƒ¦ãƒ¼ãƒ¢ã‚¢ã‚‚äº¤ãˆãªãŒã‚‰ã€å›ç­”ã¯ç¾å ´ã§å®Ÿè·µã§ãã‚‹å†…å®¹ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚
    """
}

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰
@st.cache_resource
def load_model():
    return SentenceTransformer('sonoisa/sentence-bert-base-ja-mean-tokens')

@st.cache_data
def load_knowledge():
    try:
        df = pd.read_csv("data/knowledge.csv")
        return df
    except FileNotFoundError:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯åˆ¥ã®ãƒ‘ã‚¹ã‚’è©¦ã™
        try:
            df = pd.read_csv("knowledge.csv")
            return df
        except FileNotFoundError:
            st.error("knowledge.csvãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return pd.DataFrame(columns=['ç¾è±¡', 'åŸå› ', 'å¯¾ç­–', 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'])

# ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ï¼ˆäº‹å‰è¨ˆç®—ï¼‰
@st.cache_data
def compute_embeddings(df, _model):
    # ç¾è±¡ã¨åŸå› ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    texts = df['ç¾è±¡'] + " " + df['åŸå› ']
    embeddings = _model.encode(texts)
    return embeddings

# Gemini APIè¨­å®š
API_KEY = os.environ.get('GEMINI_API_KEY')
gemini_model = None

if API_KEY:
    genai.configure(api_key=API_KEY)
    try:
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¡¨ç¤ºï¼ˆãƒ‡ãƒ—ãƒ­ã‚¤å¾Œã«ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ï¼‰
        st.sidebar.write("API_KEYè¨­å®š: OK")
        
        # ãƒ¢ãƒ‡ãƒ«å–å¾—ã‚’è©¦ã¿ã‚‹ - è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«åã§è©¦ã™
        model_names_to_try = [
            'models/gemini-1.5-pro',
            'gemini-1.5-pro',
            'models/gemini-pro',
            'gemini-pro',
            'models/gemini-1.0-pro',
            'gemini-1.0-pro'
        ]
        
        exception_messages = []
        for model_name in model_names_to_try:
            try:
                st.sidebar.write(f"ãƒ¢ãƒ‡ãƒ« {model_name} ã‚’è©¦ã—ã¦ã„ã¾ã™...")
                gemini_model = genai.GenerativeModel(model_name)
                st.sidebar.success(f"ãƒ¢ãƒ‡ãƒ« {model_name} ã®èª­ã¿è¾¼ã¿æˆåŠŸï¼")
                break
            except Exception as e:
                exception_messages.append(f"{model_name}: {str(e)}")
                continue
        
        if gemini_model is None:
            st.sidebar.error("ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«åã§å¤±æ•—ã—ã¾ã—ãŸ")
            st.sidebar.error("\n".join(exception_messages))
                
    except Exception as e:
        st.sidebar.error(f"Gemini APIæ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}")
else:
    st.sidebar.warning("âš ï¸ GEMINI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚LLMæ©Ÿèƒ½ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚")

# LLMå›ç­”ç”Ÿæˆ
@st.cache_data
def get_llm_response(query, context_data, _model, character_type):
    if not API_KEY or _model is None:
        return "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ã‹ã€ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ãŸãŸã‚ã€LLMæ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚"
    
    # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯çœŸé¢ç›®ï¼‰
    character_setting = character_prompts.get(character_type, character_prompts["çœŸé¢ç›®"])
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    prompt = f"""
    {character_setting}
    
    ä»¥ä¸‹ã®é¡ä¼¼äº‹ä¾‹ã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å…·ä½“çš„ã«ç­”ãˆã¦ãã ã•ã„ã€‚
    
    ã€å‚è€ƒæƒ…å ±ã€‘
    {context_data}
    
    ã€è³ªå•ã€‘
    {query}
    
    ã€å›ç­”ã€‘
    """
    
    try:
        # ãƒ‡ãƒãƒƒã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        st.sidebar.write(f"LLM APIã‚’å‘¼ã³å‡ºã—ä¸­... ({character_type}ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼)")
        
        # ã‚»ãƒ¼ãƒ•ãƒ†ã‚£è¨­å®š
        safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_ONLY_HIGH",
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "threshold": "BLOCK_ONLY_HIGH",
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_ONLY_HIGH",
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_ONLY_HIGH",
            },
        ]
        
        # å¿œç­”ç”Ÿæˆã‚’è©¦ã¿ã‚‹
        response = _model.generate_content(
            prompt,
            safety_settings=safety_settings,
            generation_config={"temperature": 0.7}  # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ€§ã‚’å‡ºã™ãŸã‚ã«å°‘ã—æ¸©åº¦ã‚’ä¸Šã’ã‚‹
        )
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        st.sidebar.success("LLM APIå‘¼ã³å‡ºã—æˆåŠŸ")
        
        # APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å½¢å¼ãŒå¤‰ã‚ã£ã¦ã„ã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã®ã§ã€æŸ”è»Ÿã«å¯¾å¿œ
        if hasattr(response, 'text'):
            return response.text
        elif hasattr(response, 'parts'):
            return ''.join([part.text for part in response.parts])
        else:
            return str(response)
    except Exception as e:
        st.sidebar.error(f"LLM APIã‚¨ãƒ©ãƒ¼: {str(e)}")
        return f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚AIã‚¢ãƒ‰ãƒã‚¤ã‚¹ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

# ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¨˜éŒ²ã™ã‚‹é–¢æ•°ï¼ˆå°†æ¥çš„ã«DBé€£æºãªã©ã‚’æƒ³å®šï¼‰
def log_feedback(item_id, feedback_type, query, response):
    """
    ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¨˜éŒ²ã™ã‚‹é–¢æ•°ï¼ˆç¾åœ¨ã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…ã®ã¿ï¼‰
    å°†æ¥çš„ã«ã¯CSVã€DBã€Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆãªã©ã«è¨˜éŒ²ã™ã‚‹ã“ã¨ã‚’æƒ³å®š
    """
    if 'feedback_history' not in st.session_state:
        st.session_state.feedback_history = []
    
    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æƒ…å ±ã‚’è¨˜éŒ²
    feedback_data = {
        'item_id': item_id,
        'feedback': feedback_type,
        'query': query,
        'response': response,
        'timestamp': pd.Timestamp.now()
    }
    
    st.session_state.feedback_history.append(feedback_data)
    return len(st.session_state.feedback_history)

def main():
    st.title("åˆ‡å‰ŠåŠ å·¥ãƒŠãƒ¬ãƒƒã‚¸ AI")
    st.markdown("#### ç¾å ´ã®ç–‘å•ã‚’AIãŒè§£æ±ºã—ã¾ã™")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼é¸æŠã‚’è¿½åŠ 
    st.sidebar.header("AIã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š")
    character = st.sidebar.selectbox(
        "AIã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’é¸æŠ",
        ["çœŸé¢ç›®", "ãƒ•ãƒ©ãƒ³ã‚¯", "ãŠå§‰ã•ã‚“", "å³ã—ã„å…ˆè¼©", "é–¢è¥¿å¼"]
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
    model = load_model()
    knowledge_df = load_knowledge()
    
    # ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¨ˆç®—
    if not knowledge_df.empty:
        knowledge_embeddings = compute_embeddings(knowledge_df, model)
    
        # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        st.sidebar.header("æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
        categories = ["ã™ã¹ã¦"] + list(knowledge_df["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"].str.split(",").explode().unique())
        selected_category = st.sidebar.selectbox("ã‚«ãƒ†ã‚´ãƒªã§çµã‚Šè¾¼ã¿", categories)
        
        # ã‚ˆãæ¤œç´¢ã•ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        common_queries = ["ã³ã³ã‚ŠæŒ¯å‹•", "å·¥å…·å¯¿å‘½", "è¡¨é¢ç²—ã•", "åˆ‡ã‚Šããš"]
        st.sidebar.header("ã‚ˆãèª¿ã¹ã‚‰ã‚Œã‚‹å†…å®¹")
        for query in common_queries:
            if st.sidebar.button(query):
                st.session_state.query = query  # ã‚¯ãƒªãƒƒã‚¯ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚»ãƒƒãƒˆ
        
        # ãƒ¡ã‚¤ãƒ³æ¤œç´¢ã‚¨ãƒªã‚¢
        col1, col2 = st.columns([3, 1])
        with col1:
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ãŒã‚ã‚Œã°åˆ©ç”¨
            if 'query' not in st.session_state:
                st.session_state.query = ""
            
            query = st.text_input("è³ªå•ã‚’å…¥åŠ›ï¼ˆä¾‹ï¼šå·¥å…·ãŒã™ãã«æŠ˜ã‚Œã‚‹ã€ä»•ä¸Šã’é¢ãŒç²—ã„ãªã©ï¼‰", st.session_state.query)
        
        with col2:
            search_button = st.button("æ¤œç´¢", use_container_width=True)
        
        # æ¤œç´¢å‡¦ç†
        if search_button or ('query' in st.session_state and st.session_state.query != ""):
            if query.strip():
                # ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
                query_embedding = model.encode([query])
                
                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
                similarities = cosine_similarity(query_embedding, knowledge_embeddings)[0]
                
                # ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨ï¼ˆã€Œã™ã¹ã¦ã€ä»¥å¤–ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
                if selected_category != "ã™ã¹ã¦":
                    # ã‚«ãƒ†ã‚´ãƒªã«ä¸€è‡´ã™ã‚‹è¡Œã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    filtered_indices = knowledge_df["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"].str.contains(selected_category)
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ãªã„è¡Œã®é¡ä¼¼åº¦ã‚’0ã«ã™ã‚‹
                    for i in range(len(similarities)):
                        if not filtered_indices.iloc[i]:
                            similarities[i] = 0
                
                # ä¸Šä½3ä»¶ã®é¡ä¼¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                top_indices = np.argsort(similarities)[::-1][:3]
                
                # LLMé€£æº - ä¸Šä½ã®æ¤œç´¢çµæœã‚’LLMã¸ã®æ–‡è„ˆã¨ã—ã¦ä½¿ç”¨
                context_data = ""
                for idx in top_indices[:2]:  # ä¸Šä½2ä»¶ã‚’æ–‡è„ˆã¨ã—ã¦ä½¿ç”¨
                    if similarities[idx] > 0.3:
                        context_data += f"äº‹ä¾‹: {knowledge_df.iloc[idx]['ç¾è±¡']}\n"
                        context_data += f"åŸå› : {knowledge_df.iloc[idx]['åŸå› ']}\n"
                        context_data += f"å¯¾ç­–: {knowledge_df.iloc[idx]['å¯¾ç­–']}\n\n"
                
                # LLMã§å›ç­”ç”Ÿæˆï¼ˆæ¤œç´¢çµæœãŒååˆ†ã‚ã‚‹ã¨ãï¼‰
                if context_data and API_KEY and gemini_model is not None:
                    with st.spinner("AIå›ç­”ã‚’ç”Ÿæˆä¸­..."):
                        llm_response = get_llm_response(query, context_data, gemini_model, character)
                        
                        # LLMå›ç­”ã‚’è¡¨ç¤º
                        st.markdown("## AIã‚¢ãƒ‰ãƒã‚¤ã‚¹")
                        st.markdown(llm_response)
                        st.caption(f"â€»ã“ã®å›ç­”ã¯ {character} ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã«ã‚ˆã‚‹å›ç­”ã§ã™ã€‚æœ€çµ‚åˆ¤æ–­ã¯å°‚é–€å®¶ã«ç›¸è«‡ã—ã¦ãã ã•ã„ã€‚")
                
                # æ¤œç´¢çµæœã‚’ã‚«ãƒ¼ãƒ‰UIã§è¡¨ç¤º
                st.subheader("æ¤œç´¢çµæœ")
                found_results = False
                for i, idx in enumerate(top_indices):
                    if similarities[idx] > 0.3:  # é¡ä¼¼åº¦ã—ãã„å€¤
                        found_results = True
                        with st.expander(f"{knowledge_df.iloc[idx]['ç¾è±¡']} (é¡ä¼¼åº¦: {similarities[idx]:.2f})", expanded=True if i==0 else False):
                            cols = st.columns(3)
                            with cols[0]:
                                st.markdown("### ç¾è±¡")
                                st.write(knowledge_df.iloc[idx]['ç¾è±¡'])
                            with cols[1]:
                                st.markdown("### åŸå› ")
                                st.write(knowledge_df.iloc[idx]['åŸå› '])
                            with cols[2]:
                                st.markdown("### å¯¾ç­–")
                                st.write(knowledge_df.iloc[idx]['å¯¾ç­–'])
                            
                            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒœã‚¿ãƒ³
                            feedback_cols = st.columns([1,1,4])
                            with feedback_cols[0]:
                                if st.button("ğŸ‘ å½¹ç«‹ã£ãŸ", key=f"useful_{idx}"):
                                    count = log_feedback(idx, "useful", query, knowledge_df.iloc[idx]['å¯¾ç­–'])
                                    st.success(f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼(#{count})")
                            with feedback_cols[1]:
                                if st.button("ğŸ‘ å½¹ç«‹ãŸãªã‹ã£ãŸ", key=f"not_useful_{idx}"):
                                    count = log_feedback(idx, "not_useful", query, knowledge_df.iloc[idx]['å¯¾ç­–'])
                                    st.error(f"ã”æ„è¦‹ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚æ”¹å–„ã«åŠªã‚ã¾ã™ã€‚(#{count})")
                
                if not found_results:
                    st.warning("ååˆ†ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            else:
                st.info("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    else:
        st.error("ãƒŠãƒ¬ãƒƒã‚¸ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã€‚CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´ã®è¡¨ç¤ºï¼ˆé–‹ç™ºãƒ»ãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    if st.sidebar.checkbox("ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´ã‚’è¡¨ç¤º", value=False):
        if 'feedback_history' in st.session_state and st.session_state.feedback_history:
            st.sidebar.write("### ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´")
            for i, feedback in enumerate(st.session_state.feedback_history):
                st.sidebar.write(f"**#{i+1}** - {feedback['timestamp']}")
                st.sidebar.write(f"è³ªå•: {feedback['query']}")
                st.sidebar.write(f"è©•ä¾¡: {feedback['feedback']}")
        else:
            st.sidebar.write("ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“")

if __name__ == "__main__":
    main()