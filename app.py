# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# pip install sentence-transformers scikit-learn streamlit pandas

import streamlit as st
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰
@st.cache_resource
def load_model():
    return SentenceTransformer('sonoisa/sentence-bert-base-ja-mean-tokens')

@st.cache_data
def load_knowledge():
    df = pd.read_csv("data/knowledge.csv")
    return df

# ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ï¼ˆäº‹å‰è¨ˆç®—ï¼‰
@st.cache_data
def compute_embeddings(df, model):
    # ç¾è±¡ã¨åŸå› ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    texts = df['ç¾è±¡'] + " " + df['åŸå› ']
    embeddings = model.encode(texts)
    return embeddings

def main():
    st.title("åˆ‡å‰ŠåŠ å·¥ãƒŠãƒ¬ãƒƒã‚¸ AIï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ç‰ˆï¼‰")

    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰
    model = load_model()
    knowledge_df = load_knowledge()
    knowledge_embeddings = compute_embeddings(knowledge_df, model)

    # æ¤œç´¢ã‚¯ã‚¨ãƒªå…¥åŠ›æ¬„
    query = st.text_input("è³ªå•ã‚’å…¥åŠ›ï¼ˆä¾‹ï¼šå·¥å…·ãŒã™ãã«æŠ˜ã‚Œã‚‹ã€ä»•ä¸Šã’é¢ãŒç²—ã„ãªã©ï¼‰", "")

    if st.button("æ¤œç´¢"):
        if query.strip():
            # ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            query_embedding = model.encode([query])
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
            similarities = cosine_similarity(query_embedding, knowledge_embeddings)[0]
            
            # ä¸Šä½3ä»¶ã®é¡ä¼¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            top_indices = np.argsort(similarities)[::-1][:3]
            
            # çµæœè¡¨ç¤º
            for i, idx in enumerate(top_indices):
                similarity = similarities[idx]
                if similarity > 0.3:  # é¡ä¼¼åº¦ã—ãã„å€¤
                    st.write(f"**æ¤œç´¢çµæœ {i+1}** (é¡ä¼¼åº¦: {similarity:.2f})")
                    st.write(f"**ç¾è±¡**: {knowledge_df.iloc[idx]['ç¾è±¡']}")
                    st.write(f"**åŸå› **: {knowledge_df.iloc[idx]['åŸå› ']}")
                    st.write(f"**å¯¾ç­–**: {knowledge_df.iloc[idx]['å¯¾ç­–']}")
                    st.write("---")
                else:
                    if i == 0:
                        st.warning("ååˆ†ã«é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    break
        else:
            st.info("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    main()
# ä¸Šè¨˜ã‚³ãƒ¼ãƒ‰ã«è¿½åŠ ãƒ»å¤‰æ›´

def main():
    st.set_page_config(page_title="åˆ‡å‰ŠåŠ å·¥ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼", layout="wide")
    
    st.title("åˆ‡å‰ŠåŠ å·¥ãƒŠãƒ¬ãƒƒã‚¸ AI")
    st.markdown("#### ç¾å ´ã®ç–‘å•ã‚’AIãŒè§£æ±ºã—ã¾ã™")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    st.sidebar.header("æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    categories = ["ã™ã¹ã¦"] + list(knowledge_df["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"].str.split(",").explode().unique())
    selected_category = st.sidebar.selectbox("ã‚«ãƒ†ã‚´ãƒªã§çµã‚Šè¾¼ã¿", categories)
    
    # ã‚ˆãæ¤œç´¢ã•ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä»®ã®ãƒ‡ãƒ¼ã‚¿ã€å¾Œã§å®Ÿéš›ã®æ¤œç´¢å±¥æ­´ã‹ã‚‰ç”Ÿæˆã™ã‚‹ï¼‰
    common_queries = ["ã³ã³ã‚ŠæŒ¯å‹•", "å·¥å…·å¯¿å‘½", "è¡¨é¢ç²—ã•", "åˆ‡ã‚Šããš"]
    st.sidebar.header("ã‚ˆãèª¿ã¹ã‚‰ã‚Œã‚‹å†…å®¹")
    for query in common_queries:
        if st.sidebar.button(query):
            st.session_state.query = query  # ã‚¯ãƒªãƒƒã‚¯ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚»ãƒƒãƒˆ
    
    # ãƒ¡ã‚¤ãƒ³æ¤œç´¢ã‚¨ãƒªã‚¢
    col1, col2 = st.columns([3, 1])
    with col1:
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ãŒã‚ã‚Œã°åˆ©ç”¨
        if 'query' not in st.session_state:
            st.session_state.query = ""
        
        query = st.text_input("è³ªå•ã‚’å…¥åŠ›ï¼ˆä¾‹ï¼šå·¥å…·ãŒã™ãã«æŠ˜ã‚Œã‚‹ã€ä»•ä¸Šã’é¢ãŒç²—ã„ãªã©ï¼‰", st.session_state.query)
    
    with col2:
        search_button = st.button("æ¤œç´¢", use_container_width=True)
    
    # æ¤œç´¢å‡¦ç†
    if search_button or ('query' in st.session_state and st.session_state.query != ""):
        if query.strip():
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å‡¦ç†ï¼ˆå‰è¿°ã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ï¼‰
            # ...
            
            # çµæœã‚’ã‚«ãƒ¼ãƒ‰UIã§è¡¨ç¤º
            st.subheader("æ¤œç´¢çµæœ")
            for i, idx in enumerate(top_indices):
                if similarities[idx] > 0.3:
                    with st.expander(f"{knowledge_df.iloc[idx]['ç¾è±¡']} (é¡ä¼¼åº¦: {similarities[idx]:.2f})", expanded=True if i==0 else False):
                        cols = st.columns(3)
                        with cols[0]:
                            st.markdown("### ç¾è±¡")
                            st.write(knowledge_df.iloc[idx]['ç¾è±¡'])
                        with cols[1]:
                            st.markdown("### åŸå› ")
                            st.write(knowledge_df.iloc[idx]['åŸå› '])
                        with cols[2]:
                            st.markdown("### å¯¾ç­–")
                            st.write(knowledge_df.iloc[idx]['å¯¾ç­–'])
                        
                        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒœã‚¿ãƒ³
                        feedback_cols = st.columns([1,1,4])
                        with feedback_cols[0]:
                            st.button("ğŸ‘ å½¹ç«‹ã£ãŸ", key=f"useful_{idx}")
                        with feedback_cols[1]:
                            st.button("ğŸ‘ å½¹ç«‹ãŸãªã‹ã£ãŸ", key=f"not_useful_{idx}")
# pip install google-generativeai

import google.generativeai as genai
import os

# Gemini APIè¨­å®š
API_KEY = "ã‚ãªãŸã®Gemini APIã‚­ãƒ¼"  # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã™ã‚‹æ–¹ãŒå®‰å…¨ã§ã™
genai.configure(api_key=API_KEY)

# ãƒ¢ãƒ‡ãƒ«è¨­å®š
model = genai.GenerativeModel('gemini-pro')

@st.cache_data
def get_llm_response(query, context_data):
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    prompt = f"""
    ã‚ãªãŸã¯åˆ‡å‰ŠåŠ å·¥ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã™ã€‚ä»¥ä¸‹ã®é¡ä¼¼äº‹ä¾‹ã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å…·ä½“çš„ã«ç­”ãˆã¦ãã ã•ã„ã€‚
    å›ç­”ã¯ç°¡æ½”ã«ã€ç¾å ´ã§å®Ÿè·µã§ãã‚‹å†…å®¹ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚
    
    ã€å‚è€ƒæƒ…å ±ã€‘
    {context_data}
    
    ã€è³ªå•ã€‘
    {query}
    
    ã€å›ç­”ã€‘
    """
    
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        st.error(f"APIã‚¨ãƒ©ãƒ¼: {e}")
        return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ä¸€æ™‚çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

def main():
    # æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¯åŒã˜...
    
    # æ¤œç´¢å‡¦ç†
    if search_button or ('query' in st.session_state and st.session_state.query != ""):
        if query.strip():
            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å‡¦ç†
            # ...
            
            # ä¸Šä½ã®æ¤œç´¢çµæœã‚’LLMã¸ã®æ–‡è„ˆã¨ã—ã¦ä½¿ç”¨
            context_data = ""
            for idx in top_indices[:2]:  # ä¸Šä½2ä»¶ã‚’æ–‡è„ˆã¨ã—ã¦ä½¿ç”¨
                if similarities[idx] > 0.3:
                    context_data += f"äº‹ä¾‹: {knowledge_df.iloc[idx]['ç¾è±¡']}\n"
                    context_data += f"åŸå› : {knowledge_df.iloc[idx]['åŸå› ']}\n"
                    context_data += f"å¯¾ç­–: {knowledge_df.iloc[idx]['å¯¾ç­–']}\n\n"
            
            # LLMã§å›ç­”ç”Ÿæˆï¼ˆæ¤œç´¢çµæœãŒååˆ†ã‚ã‚‹ã¨ãï¼‰
            if context_data:
                with st.spinner("AIå›ç­”ã‚’ç”Ÿæˆä¸­..."):
                    llm_response = get_llm_response(query, context_data)
                    
                    # LLMå›ç­”ã‚’è¡¨ç¤º
                    st.markdown("## AIã‚¢ãƒ‰ãƒã‚¤ã‚¹")
                    st.markdown(llm_response)
                    st.caption("â€»ã“ã®å›ç­”ã¯å‚è€ƒæƒ…å ±ã§ã™ã€‚æœ€çµ‚åˆ¤æ–­ã¯å°‚é–€å®¶ã«ç›¸è«‡ã—ã¦ãã ã•ã„ã€‚")
                    
            # ä»¥ä¸‹ã€æ¤œç´¢çµæœã®è¡¨ç¤ºã¯åŒã˜...